{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am using Apple Silicon, Metal Performance Shaders is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# use cuda if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_file = 'traces.csv'\n",
    "nrows = 1000 # number of traces to use (debug purposes)\n",
    "batch_size = 50\n",
    "print('number of traces: ', len(pd.read_csv(traces_file)))\n",
    "print('number of traces used: ', nrows)\n",
    "\n",
    "traces = pd.read_csv(traces_file, nrows=nrows)\n",
    "# Split into train and test\n",
    "train_data, test_data = train_test_split(traces, test_size=0.25)\n",
    "\n",
    "# get number of unique values in each column\n",
    "print('number of unique values to predict: ', traces.nunique()[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, batch_size):\n",
    "    label_encoder_pc = LabelEncoder()\n",
    "    label_encoder_delta_in = LabelEncoder()\n",
    "    label_encoder_delta_out = LabelEncoder()\n",
    "\n",
    "    # Fit label encoder and transform labels into encoded values\n",
    "    data['pc_encoded'] = label_encoder_pc.fit_transform(data['pc'])\n",
    "    data['delta_in_encoded'] = label_encoder_delta_in.fit_transform(data['delta_in'])\n",
    "    data['delta_out_encoded'] = label_encoder_delta_out.fit_transform(data['delta_out'])\n",
    "\n",
    "    # Convert dataframes to tensors\n",
    "    pc = torch.tensor(data['pc_encoded'].values)\n",
    "    delta_in = torch.tensor(data['delta_in_encoded'].values)\n",
    "    targets = torch.tensor(data['delta_out_encoded'].values)\n",
    "\n",
    "    # Create a custom Dataset instance\n",
    "    dataset = TensorDataset(pc, delta_in, targets)\n",
    "\n",
    "    # Create a DataLoader instance\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Get unique target keys\n",
    "    target_keys = set(data['delta_out_encoded'].unique())\n",
    "\n",
    "    return data_loader, len(label_encoder_pc.classes_), len(label_encoder_delta_in.classes_), len(label_encoder_delta_out.classes_), target_keys\n",
    "\n",
    "train_iter, num_pc, num_delta_in, num_output_next, target_keys = load_data(train_data, batch_size=batch_size)\n",
    "test_iter, _, _, _, _ = load_data(test_data, batch_size=batch_size)\n",
    "# add 1 for the next delta which is not in the training set\n",
    "num_pc += 1\n",
    "num_delta_in += 1\n",
    "num_output_next += 1\n",
    "\n",
    "print('number of unique pc: ', num_pc)\n",
    "print('number of unique input delta: ', num_delta_in)\n",
    "print('number of unique output delta: ', num_output_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLSTM(nn.Module):\n",
    "    def __init__(self, num_pc, num_delta_in, num_output_next, embed_dim, hidden_dim, topPredNum, num_layers, dropout):\n",
    "        # Layer structure is described in the paper\n",
    "        super(EmbeddingLSTM, self).__init__()\n",
    "        self.topPredNum = topPredNum\n",
    "        # Define embedding layers\n",
    "        self.pc_embed_layer = nn.Embedding(num_pc, embed_dim)\n",
    "        self.delta_embed_layer = nn.Embedding(num_delta_in, embed_dim)\n",
    "        # Define LSTM layer\n",
    "        # LSTM input dimension: (pc_embed + delta_embed) * 2\n",
    "        lstm_input_dim = embed_dim * 2\n",
    "        self.lstm = nn.LSTM(lstm_input_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "        # Define output layer\n",
    "        self.fc = nn.Linear(hidden_dim, num_output_next)\n",
    "        # Define dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, h_c_state, target=None):\n",
    "        pc, delta = x\n",
    "        # pc goes through pc embedding layer\n",
    "        # delta goes through delta embedding layer\n",
    "        pc_embed_layer = self.pc_embed_layer(pc)\n",
    "        delta_embed_layer = self.delta_embed_layer(delta)\n",
    "        # Concatenate pc and delta embedding layers\n",
    "        # The concatenated layer is the input to the LSTM layer\n",
    "        pc_delta_embed_out = torch.cat([pc_embed_layer, delta_embed_layer], dim = -1)\n",
    "        lstm_out, state = self.lstm(pc_delta_embed_out, h_c_state)\n",
    "        # lstm_out shape: (batch_size, seq_len, hidden_dim)\n",
    "        outputs = self.dropout(self.fc(lstm_out))\n",
    "        ####### Embedding LSTM layers constructed #######\n",
    "\n",
    "        ####### Get top k predictions #######\n",
    "        delta_probabilities = nn.functional.log_softmax(outputs, dim = -1).squeeze(dim = 1)\n",
    "        # Get top k predictions\n",
    "        _, preds = torch.topk(delta_probabilities, self.topPredNum, sorted=False)\n",
    "\n",
    "        if target is not None:\n",
    "            loss = nn.functional.cross_entropy(delta_probabilities, target) \n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return preds, state, loss\n",
    "\n",
    "    def predict(self, X, lstm_state):\n",
    "        with torch.no_grad():\n",
    "            preds, state, _ = self.forward(X, lstm_state)\n",
    "            return preds, state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hyperparameters\n",
    "hparams = {\n",
    "    \"topPredNum\": 5,\n",
    "    \"embed_dim\": 256,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"epochs\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with hyperparameters\n",
    "my_model = EmbeddingLSTM(\n",
    "    num_pc,\n",
    "    num_delta_in,\n",
    "    num_output_next,\n",
    "    hparams[\"embed_dim\"],\n",
    "    hparams[\"hidden_dim\"],\n",
    "    topPredNum=hparams[\"topPredNum\"],\n",
    "    num_layers=hparams[\"num_layers\"],\n",
    "    dropout=hparams[\"dropout\"]\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "train_loss = []\n",
    "my_model = my_model.to(device)\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=hparams[\"learning_rate\"])\n",
    "\n",
    "# Start training\n",
    "for epoch in range(hparams[\"epochs\"]):\n",
    "    # Switch to training mode\n",
    "    my_model.train()\n",
    "    lstm_state = None\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        batch = [ds.to(device) for ds in batch]\n",
    "        inputs = batch[:-1]\n",
    "        targets = batch[-1]\n",
    "        _, lstm_state, batch_loss = my_model(inputs, lstm_state, targets)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(float(batch_loss.detach()))\n",
    "        print(f\"Epoch {epoch + 1}, Iteration {idx + 1}, Loss: {train_loss[-1]:.8f}\")\n",
    "        # Remove state gradients to prevent autograd errors\n",
    "        lstm_state = tuple([s.detach() for s in lstm_state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(network, data_iterator, relevant_keys, computing_device=\"cpu\", initial_state=None):\n",
    "    network.eval()\n",
    "\n",
    "    accuracy_metrics = [process_batch(i, batch_data, network, computing_device, initial_state, relevant_keys) \n",
    "                        for i, batch_data in enumerate(data_iterator)]\n",
    "\n",
    "    average_accuracy = torch.tensor(accuracy_metrics).mean()\n",
    "    print(\"Average Validation Accuracy: {:.4f}\".format(average_accuracy))\n",
    "\n",
    "def process_batch(batch_index, batch_data, network, device, state, keys):\n",
    "    print(f\"Processing batch {batch_index}\")\n",
    "\n",
    "    batch_data = [item.to(device) for item in batch_data]\n",
    "    input_data = batch_data[:-1]\n",
    "    labels = batch_data[-1]\n",
    "\n",
    "    predictions, state = network.predict(input_data, state)\n",
    "\n",
    "    accuracy = compute_accuracy(predictions, labels, keys)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def compute_accuracy(predictions, labels, keys):\n",
    "    combined_data = list(zip(labels, predictions))\n",
    "    count_correct = sum([1 for label, predicted in combined_data \n",
    "                         if label.item() in keys and label in predicted])\n",
    "    \n",
    "    return count_correct / len(labels)\n",
    "\n",
    "validate_model(my_model, test_iter, target_keys, computing_device=\"mps\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
